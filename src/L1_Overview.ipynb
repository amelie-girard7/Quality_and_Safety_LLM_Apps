{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this course is to Find metrics to Evaluate LLMs \n",
    "<img source=\"\">\n",
    "[](./img/metrics.png)\n",
    "\n",
    "This code will:\n",
    "1. Explore the dataset of LLM prompts and responses named **chats.csv** that weâ€™ll use throughout this course.\n",
    "2. Get a fast demo overview of all the techniques showcased in greater detail in later lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Agenda \n",
    "\n",
    "1. **Introduction and Dataset Exploration**\n",
    "   - Initially, the code aims to import necessary libraries and explore a dataset containing prompts and responses from a large language model (LLM). The dataset is provided in a CSV file named **chats.csv**. The first few rows of the dataset are displayed to get an overview of its structure and contents.\n",
    "\n",
    "2. **Setup and explore whylogs and langkit**\n",
    "   - The code then sets up tools for analyzing the dataset. It uses `whylogs` for logging and analyzing dataset statistics and `langkit` for specific metrics related to large language models. The dataset is logged using these tools to track various aspects like prompt-response relevance.\n",
    "\n",
    "3. **Analyzing Prompt-Response Relevance**\n",
    "   - The relevance of the LLM's responses to the prompts is analyzed. This involves using custom functions to visualize and identify how relevant each response is to its corresponding prompt.\n",
    "\n",
    "4. **Data Leakage Analysis**\n",
    "   - The code performs an analysis to detect any patterns in the prompts and responses that might lead to data leakage, which could compromise the model's integrity or reveal sensitive information.\n",
    "\n",
    "5. **Toxicity Analysis**\n",
    "   - To ensure the content's appropriateness, the code analyzes the dataset for toxicity in both prompts and responses.\n",
    "\n",
    "6. **Injection Analysis**\n",
    "   - This part of the code checks for any injections in the dataset. Injections can be malicious inputs or errors that could potentially harm the model or skew the analysis.\n",
    "\n",
    "7. **Evaluating Specific Cases**\n",
    "   - Finally, the code filters and evaluates specific cases within the dataset. For instance, it looks at responses containing apologies or prompts exceeding a certain length, assessing these subsets for quality or other criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Dataset Exploration\n",
    "\n",
    "Next, we load a CSV file into a DataFrame. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\24419222_admin\\OneDrive - UTS\\Documents\\GitHub\\Quality_and_Safety_LLM_Apps\\src\\L1_Overview.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhelpers\u001b[39;00m  \u001b[39m# This is likely a custom module for specific helper functions.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m  \u001b[39m# Pandas is a popular data manipulation library.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m chats \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./chats.csv\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Loading the dataset from 'chats.csv' into 'chats' DataFrame.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import helpers  # This is likely a custom module for specific helper functions.\n",
    "import pandas as pd  # Pandas is a popular data manipulation library.\n",
    "\n",
    "chats = pd.read_csv(\"./chats.csv\")  # Loading the dataset from 'chats.csv' into 'chats' DataFrame.\n",
    "chats.head(5)  # Displaying the first 5 rows of the DataFrame for a quick preview.\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # Setting pandas to display full content of each column.\n",
    "chats.head(5)  # Displaying the first 5 rows again, this time showing full column content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and explore whylogs and langkit\n",
    "\n",
    "Setup and Explore whylogs and langkit: These are tools for logging and analyzing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whylogs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\24419222_admin\\OneDrive - UTS\\Documents\\GitHub\\Quality_and_Safety_LLM_Apps\\src\\L1_Overview.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwhylogs\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mwhy\u001b[39;00m  \u001b[39m# Importing the whylogs library for logging dataset statistics.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m why\u001b[39m.\u001b[39minit(\u001b[39m\"\u001b[39m\u001b[39mwhylabs_anonymous\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Initializing whylogs with an anonymous configuration.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/24419222_admin/OneDrive%20-%20UTS/Documents/GitHub/Quality_and_Safety_LLM_Apps/src/L1_Overview.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangkit\u001b[39;00m \u001b[39mimport\u001b[39;00m llm_metrics  \u001b[39m# Importing a module from langkit for LLM metrics.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'whylogs'"
     ]
    }
   ],
   "source": [
    "import whylogs as why  # Importing the whylogs library for logging dataset statistics.\n",
    "why.init(\"whylabs_anonymous\")  # Initializing whylogs with an anonymous configuration.\n",
    "\n",
    "from langkit import llm_metrics  # Importing a module from langkit for LLM metrics.\n",
    "schema = llm_metrics.init()  # Initializing a metrics schema for logging.\n",
    "\n",
    "result = why.log(chats, name=\"LLM chats dataset\", schema=schema)  # Logging the chats dataset with the defined schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Prompt-Response Relevance\n",
    "\n",
    "Using custom functions to visualize and analyze the relevance of responses to prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import input_output  # Importing a module from langkit for input-output analysis.\n",
    "\n",
    "# Visualizing a specific metric related to the relevance of responses to prompts.\n",
    "helpers.visualize_langkit_metric(chats, \"response.relevance_to_prompt\")\n",
    "\n",
    "# Displaying critical queries regarding response relevance.\n",
    "helpers.show_langkit_critical_queries(chats, \"response.relevance_to_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage Analysis\n",
    "\n",
    "Identifying patterns in prompts and responses that might lead to data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import regexes  # Importing a module for regex operations.\n",
    "\n",
    "# Visualizing metrics to identify patterns in prompts and responses.\n",
    "helpers.visualize_langkit_metric(chats, \"prompt.has_patterns\")\n",
    "helpers.visualize_langkit_metric(chats, \"response.has_patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Analysis\n",
    "\n",
    "Evaluating prompts and responses for toxic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import toxicity  # Importing a module for toxicity analysis.\n",
    "\n",
    "# Visualizing metrics related to toxicity in prompts and responses.\n",
    "helpers.visualize_langkit_metric(chats, \"prompt.toxicity\")\n",
    "helpers.visualize_langkit_metric(chats, \"response.toxicity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injection Analysis\n",
    "Checking for injections in the data, which can be malicious inputs or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import injections  # Importing a module for injection analysis.\n",
    "\n",
    "# Visualizing metrics related to injections in the data.\n",
    "helpers.visualize_langkit_metric(chats, \"injection\")\n",
    "helpers.show_langkit_critical_queries(chats, \"injection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Specific Cases\n",
    "Filtering and evaluating the dataset based on specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.evaluate_examples()  # Evaluating some examples from the dataset.\n",
    "\n",
    "# Filtering chats where the response contains 'Sorry' and evaluating them.\n",
    "filtered_chats = chats[chats[\"response\"].str.contains(\"Sorry\")]\n",
    "helpers.evaluate_examples(filtered_chats)\n",
    "\n",
    "# Filtering chats where the prompt length is greater than 250 characters and evaluating them.\n",
    "filtered_chats = chats[chats[\"prompt\"].str.len() > 250]\n",
    "helpers.evaluate_examples(filtered_chats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877baeec-ea53-467b-8438-9900f7b66a61",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
